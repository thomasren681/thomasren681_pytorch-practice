import os

#文件处理的函数库

import torch

import torch.nn as nn

import torch.utils.data as Data

import torchvision

import matplotlib.pyplot as plt







EPOCH = 1

BATCH_SIZE=50

LR=0.001

DOWNLOAD_MNIST=False#是否下载mnist数据



if not(os.path.exists('./mnist/'))or not os.listdir('./mnist/'):

    DOWNLOAD_MNIST=True

    #如果mnist文件不存在，就下载。





train_data=torchvision.datasets.MNIST(

    root='./mnist/',#定位到那个根文件

    train=True,#true则创建training文件，没有则创建test文件

    transform=torchvision.transforms.ToTensor(),#将图片转化成张量形式

    download=DOWNLOAD_MNIST,#如果为true，则从internet下载数据集并将其放在根目录中。如果数据集已下载，则不会再次下载。

)





# print(train_data.train_data.size())                 # (60000, 28, 28)

#

# print(train_data.train_labels.size())               # (60000)

#

# plt.imshow(train_data.train_data[5].numpy(), cmap='gray')

#

# plt.title('%i' % train_data.train_labels[5])

#

# plt.show()



train_loader=Data.DataLoader(dataset=train_data,batch_size=BATCH_SIZE,shuffle=True)#将训练集分成打乱的minibatch



test_data=torchvision.datasets.MNIST(root='./mnist/',train=False)#把mnist中的值载入并合并成一个矩阵

# print(test_data.test_data.shape,test_data.test_labels.shape)

test_x=torch.unsqueeze(test_data.test_data,dim=1).type(torch.FloatTensor)[:2000]/255

test_y=test_data.test_labels[:2000]#选取mnist中前两千个作为测试集





#print(test_x.shape,test_y.shape)





class CNN(nn.Module):

    def __init__(self):

        super(CNN, self).__init__()

        self.conv1=nn.Sequential(

            nn.Conv2d(

                in_channels=1,

                out_channels=16,

                kernel_size=5,

                stride=1,

                padding=2,

            ),#same convolution, 16 time the channels

            nn.ReLU(),

            nn.MaxPool2d(kernel_size=2),#maxpooling half the size

        )

        self.conv2=nn.Sequential(

            nn.Conv2d(

                in_channels=16,

                out_channels=32,

                kernel_size=5,

                stride=1,

                padding=2,

            ),#same convolution, double the channels

            nn.ReLU(),

            nn.MaxPool2d(kernel_size=2),#half the size

        )

        self.out=nn.Linear(32*7*7,10)#receive a  fully connected nn,output to 10-dim rowspace tensor, corresponds to ten numbers.

    def forward(self,x):

        x=self.conv1(x)

        x=self.conv2(x)

        x=x.view(x.size(0),-1)#transform a tensor to a one_dim tensor. -1 refers to 自适应分配，指在不知道函数有多少列的情况下，根据原tensor数据自动分配列数。

        output=self.out(x)

        return output,x



cnn=CNN()

#print(cnn)





optimizer=torch.optim.Adam(cnn.parameters(),lr=LR)

loss_func=nn.CrossEntropyLoss()









from matplotlib import cm



try: from sklearn.manifold import TSNE; HAS_SK = True



except: HAS_SK = False; print('Please install sklearn for layer visualization')



def plot_with_labels(lowDWeights, labels):



    plt.cla()



    X, Y = lowDWeights[:, 0], lowDWeights[:, 1]



    for x, y, s in zip(X, Y, labels):



        c = cm.rainbow(int(255 * s / 9)); plt.text(x, y, s, backgroundcolor=c, fontsize=9)



    plt.xlim(X.min(), X.max()); plt.ylim(Y.min(), Y.max()); plt.title('Visualize last layer'); plt.show(); plt.pause(0.01)







plt.ion()





for epoch in range(EPOCH):

    for step,(b_x,b_y) in enumerate(train_loader):

        output = cnn(b_x)[0]#取返回值中的output，而不是x

        loss=loss_func(output,b_y)

        optimizer.zero_grad()

        loss.backward()

        optimizer.step()



        if step % 50 == 0:



            test_output, last_layer = cnn(test_x)



            pred_y = torch.max(test_output, 1)[1].data.numpy()



            accuracy = float((pred_y == test_y.data.numpy()).astype(int).sum()) / float(test_y.size(0))



            print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.numpy(), '| test accuracy: %.2f' % accuracy)



            if HAS_SK:

                # Visualization of trained flatten layer (T-SNE)



                tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)



                plot_only = 500



                low_dim_embs = tsne.fit_transform(last_layer.data.numpy()[:plot_only, :])



                labels = test_y.numpy()[:plot_only]



                plot_with_labels(low_dim_embs, labels)





plt.ioff()

test_output,_ =cnn(test_x[:10])

pred_y=torch.max(test_output,1)[1].data.numpy()#返回张量中每一行的最大值，即softmax中最大概率的值的索引，即其对应的1-10的值

print(pred_y)

print(test_y[:10].numpy())
